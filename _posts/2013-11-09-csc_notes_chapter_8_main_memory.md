---
layout: post
title: CSC Notes Chapter 8 Main Memory
category: booknote
---

#Main Memory

>Chapter Objectives

>* To provide a detailed description of various ways of organising memory hardware
* To discuss various memory-management techniques, including paging and segmentation
* To provide a detailed description of the Intel Pentium, which supports both pure segmentation and segmentation with paging

##Background
>Memory consists of a large array of words or bytes, each with its own address. The CPU fetches instructions from memory according to the value of the program counter. These instructions may cause additional loading from and storing to specific memory addresses.

###Basic Hardware
Main memory and the registers are the only storage that the CPU can access directly. This reality leads to 2 implications:

* Any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices.
* If the data are not in memory, they must be moved there before the CPU can operate on them.

Difference between register and memory:

* registers are built into the CPU and can be generally accessed within on cycle of the CPU clock. Most CPUs can decode instructions and perform simple operations on register contents at the rate of one or more operations per clock tick.
* Memory is accessed via a transaction on the memory bus. Completing a memory access may take many cycles of the CPU clock. 

Since accessing memory is slow, the processor needs to __stall__ when accessing memory. This is intolerable and can be solved a memory buffer called __cache__.

Besides the accessing speed, the hardware also provides protection from mis-operations on memory. Here is one of the possible mechanism:

* Each process has a separate memory space:
    * base register: the smallest legal physical memory address
    * limit register: the size of the range
* CPU compares _every_ address generated in the user mode with the registers. Any attempt by a program executing in user mode to access OS memory or other users' memory results in a trap to the OS, which treats the attempt as a fatal error.

Loading base and limit registers can only be performed by OS in kernel mode via privileged instructions.

OS, executing in kernel mode, can unrestrictedly access both OS memory and users' memory to dump out those programs in case of errors, to access and modify parameters of system calls, and so on.

###Address Binding

Processes on the disk that are waiting to be brought into memory for execution form the __input queue__.

The binding of instructions and data to memory addresses:

* Compile time: if you know at compile time where the process will reside in memory, then __absolute code__ can be generated.
* Load time: if it is not known at compile time where the process will reside in memory, then the compiler must generate __relocatable code__. In this case, final binding is delayed until load time.
* Execution time: if the process can be moved during its execution from one memory segment to another, then binging must be delayed until run time. Need special hardware. Most general-purpose OS use this method.

###Logical versus Physical Address Space

* Logical address: addresses generated by the CPU
* Physical address: addresses seen by the memory unit - that is, the one loaded into the memory-address register of the memory

The compile-time and load-time address-binding methods generate identical logical and physical addresses.

__The execution-time address-binding scheme results in differing logical and physical addresses.__ In this case, we usually refer to the logical address as a __virtual address__.

* Logical(virtual) address space: the set of all logical addresses generated by a program
* Physical address space: the set of all physical addresses corresponding to the logical addresses

In the execution-time address-binding scheme, the logical and physical address spaces differ. The mapping from virtual to physical addresses in the execution-time is done by a hardware device called __memory-management unit(MMU)__.

The user program never sees the _real_ physical addresses. The user program deals with logical addresses. MMU maps these logical addresses to real physical addresses.

###Dynamic Loading

Procedures:

* a routine is not loaded until it is called. All routines are kept on disk in a relocatable load format
* main program is loaded into memory and executed
* When a routine needs to call another routine, the calling routine first checks to see whether the other routine has been loaded
* If not, the relocatable linking loader is called to load the desired routine into memory and to update the program's address tables to reflect this change
* Then control is passed to the newly loaded routine

Advantages:

* an unused routine is never loaded
* thus the total program size may be large, the portion that is used may be much smaller

Dynamic loading is up to users' implementation of their programs and does not require special OS support. OS may help the programmer by providing library routines to implement dynamic loading.

###Dynamic Linking and Shared Libraries

* Static linking: system language libraries are treated like any other object module and are combined by the loader into the binary program image
* Dynamic linking: like dynamic loading. Loads the language routine only when it's necessary

Usage: 

* Dynamic linking avail all processes to use a language library execute only one copy of the library code.
* A library may be replaced by a new version, and all programs that reference the library will automatically use the new version.
* In case of accidentally using incompatible version of libraries, both program and library keep a version number. Programs only load the libraries with matching version number. In this case, there might be multiple versions of libraries loaded into memory. 

This system is also known as __shared libraries__.

Dynamic linking usually requires help from the OS.

##Swapping

>A process must be in memory to be executed. A process, however, can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution.

Swapping in RR scheduling:

* when a quantum expires, the memory manager will start to swap out the process that just finished
* swap in another process into the memory space that has been freed
* CPU scheduler will allocate a time slice to some other process in memory
* when the CPU scheduler wants to reschedule the CPU, there should be some processes in memory, ready to execute; it's guaranteed by:
    * memory manager swap processes fast enough
    * the quantum is large enough to allow reasonable amounts of computing to be done  between swaps




 





























